{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "import re\n",
    "from ast import literal_eval\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "import time\n",
    "\n",
    "from config import db_password \n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and import our 2 files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"Resources\"\n",
    "# Observation data\n",
    "weather_file = f'../{file_dir}/rcc_data.json'\n",
    "\n",
    "#Parameter File\n",
    "params_file = f'../{file_dir}/params_list.json'\n",
    "\n",
    "# #County File - this is a file downloaded from this government site - https://www2.census.gov/geo/docs/reference/codes/files/st27_mn_cou.txt\n",
    "# county_file = f'../{file_dir}/mn_county_ref.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the read the Observation data JSON file.\n",
    "with open(f'{weather_file}', mode='r') as file:\n",
    "    weather_raw = json.load(file)  # Load the file into a list of Dictionaries NOT RIGHT TO JSON\n",
    "\n",
    "# open the API params file\n",
    "with open(f'{params_file}', mode='r') as file:\n",
    "    params_data = json.load(file)  # Load the file into a list of Dictionaries NOT RIGHT TO JSON\n",
    "\n",
    "# # open the MN county reference\n",
    "# county_df = pd.read_csv(county_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.DataFrame(weather_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the API parameters we used into a dataframe\n",
    "# We need the parameters to know the date range and the values in the observation Array\n",
    "params_df = pd.DataFrame(params_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse out the JSON data into a Stations Dataframe and an Observations Dataframe\n",
    "counter = 0\n",
    "stations_list =[]\n",
    "observations_list = []\n",
    "\n",
    "# Convert the start/end date used in the API call stored in the params file\n",
    "startDate = pd.to_datetime(params_df['sdate'][0])\n",
    "endDate = pd.to_datetime(params_df['edate'][0])\n",
    "\n",
    "# Loop through the upper most level of the JSON file called \"data\"\n",
    "for observation in weather_raw['data']:\n",
    "\n",
    "    # Create a list of stations\n",
    "    stations_list.append(observation[\"meta\"])\n",
    "        \n",
    "    # Start the current date at the first date in the range for each stations observations\n",
    "    currentDate = startDate\n",
    "    \n",
    "    # Loop through the stations observations and create a list of observations\n",
    "    for observation_data in observation[\"data\"]:\n",
    "        observations_list.append({\"station_uid\":observation[\"meta\"]['uid'],\"date\":currentDate, \"data\":observation_data})\n",
    "\n",
    "        # Increment the date by one day\n",
    "        currentDate = currentDate + timedelta(days=1)\n",
    "\n",
    "        # Increment our counter (for debugging use)\n",
    "        counter = counter + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert our lists to dataframes\n",
    "stations_df = pd.DataFrame(stations_list)\n",
    "observations_df = pd.DataFrame(observations_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_uid</th>\n",
       "      <th>date</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3266594</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-12</td>\n",
       "      <td>[M, M, 0.96, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266595</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-13</td>\n",
       "      <td>[M, M, 0.11, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266596</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-14</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266597</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-15</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266598</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_uid       date                   data\n",
       "3266594        63155 2022-05-12  [M, M, 0.96, M, M, M]\n",
       "3266595        63155 2022-05-13  [M, M, 0.11, M, M, M]\n",
       "3266596        63155 2022-05-14     [M, M, M, M, M, M]\n",
       "3266597        63155 2022-05-15     [M, M, M, M, M, M]\n",
       "3266598        63155 2022-05-16     [M, M, M, M, M, M]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the stations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the ll list into Latitude and longitude \n",
    "split_df = pd.DataFrame(stations_df['ll'].to_list(), columns = ['latitude','longitude'])\n",
    "# concat df and split_df\n",
    "stations_df = pd.concat([stations_df, split_df], axis=1)\n",
    "# Drop ll column.\n",
    "stations_df = stations_df.drop(columns=[\"ll\"])\n",
    "# # display df\n",
    "# stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create sids columns for the array sids\n",
    "# sids = []\n",
    "# sid_string = \"\"\n",
    "# counter = 1\n",
    "# for x in range(stations_df.sids.map(len).max()):\n",
    "#   sids.append(f\"sid_{counter}\")\n",
    "#   sid_string = f\"{sid_string}, 'sid_{counter}'\"\n",
    "#   counter = counter + 1\n",
    "\n",
    "# # split the array into separate columns\n",
    "# split_df = pd.DataFrame(stations_df['sids'].to_list(), columns = sids)\n",
    "\n",
    "# # concat df and split_df\n",
    "# stations_df = pd.concat([stations_df, split_df], axis=1)\n",
    "\n",
    "# # Drop names from columns.\n",
    "stations_df = stations_df.drop(columns=[\"sids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the county field to a numeric type\n",
    "stations_df['county'] = stations_df['county'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns in the DF - Remove the valid_daterange and sid_dates since they are not necessary\n",
    "#column_list = f\"['uid','name','county','state','latitude','longitude','climdiv'{sid_string}]\"  #'valid_daterange','sid_dates'\n",
    "column_list = f\"['uid','name','county','state','latitude','longitude','climdiv']\"  #'valid_daterange','sid_dates'\n",
    "\n",
    "# change the string into an array for the reordering\n",
    "columnArray = literal_eval(column_list)\n",
    "\n",
    "stations_df= stations_df.loc[:, columnArray]\n",
    "stations_df.rename(columns={'uid':'station_uid'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the observations dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_uid</th>\n",
       "      <th>date</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3266594</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-12</td>\n",
       "      <td>[M, M, 0.96, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266595</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-13</td>\n",
       "      <td>[M, M, 0.11, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266596</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-14</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266597</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-15</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266598</th>\n",
       "      <td>63155</td>\n",
       "      <td>2022-05-16</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_uid       date                   data\n",
       "3266594        63155 2022-05-12  [M, M, 0.96, M, M, M]\n",
       "3266595        63155 2022-05-13  [M, M, 0.11, M, M, M]\n",
       "3266596        63155 2022-05-14     [M, M, M, M, M, M]\n",
       "3266597        63155 2022-05-15     [M, M, M, M, M, M]\n",
       "3266598        63155 2022-05-16     [M, M, M, M, M, M]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_uid</th>\n",
       "      <th>date</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3058251</th>\n",
       "      <td>96447</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058252</th>\n",
       "      <td>96447</td>\n",
       "      <td>2002-01-02</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058253</th>\n",
       "      <td>96447</td>\n",
       "      <td>2002-01-03</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058254</th>\n",
       "      <td>96447</td>\n",
       "      <td>2002-01-04</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058255</th>\n",
       "      <td>96447</td>\n",
       "      <td>2002-01-05</td>\n",
       "      <td>[M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_uid       date                data\n",
       "3058251        96447 2002-01-01  [M, M, M, M, M, M]\n",
       "3058252        96447 2002-01-02  [M, M, M, M, M, M]\n",
       "3058253        96447 2002-01-03  [M, M, M, M, M, M]\n",
       "3058254        96447 2002-01-04  [M, M, M, M, M, M]\n",
       "3058255        96447 2002-01-05  [M, M, M, M, M, M]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = observations_df.loc[(observations_df['station_uid'] == 96447)]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data columns into the elems requested\n",
    "elems = str(params_df[\"elems\"][0])\n",
    "\n",
    "elems = elems.replace(\",\",\"','\")\n",
    "\n",
    "# convert the elems from the params list to a string for splitting apart the Array \n",
    "column_list = f\"['{elems}']\"\n",
    "columnArray = literal_eval(column_list)\n",
    "\n",
    "# split the array into separate columns]\n",
    "split_df = pd.DataFrame(observations_df['data'].to_list(), columns = columnArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any of the \"M\" values with NaN in the data.  This is a Missing Value\n",
    "split_df.replace(\"M\",np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any of the \"T\" values with 0 in the data.  This is a trace amount of precipitation\n",
    "split_df.replace(\"T\",0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the data returned based on the Elements stored in the params json file\n",
    "\n",
    "# concat df and split_df\n",
    "observations_df = pd.concat([observations_df, split_df], axis=1)\n",
    "# Drop data column.\n",
    "observations_df = observations_df.drop(columns=[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_uid    3266599\n",
       "date           3266599\n",
       "maxt            126528\n",
       "mint            126467\n",
       "pcpn            525442\n",
       "snow            326223\n",
       "snwd            135202\n",
       "avgt            126073\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any observations where it's missing a min temperature\n",
    "observations_df = observations_df.dropna(subset=['mint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_uid    126467\n",
       "date           126467\n",
       "maxt           126073\n",
       "mint           126467\n",
       "pcpn           115512\n",
       "snow            91141\n",
       "snwd            85409\n",
       "avgt           126073\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the datatypes of the columns to numeric\n",
    "observations_df[columnArray] = observations_df[columnArray].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_uid    439\n",
       "name           439\n",
       "county         439\n",
       "state          439\n",
       "latitude       439\n",
       "longitude      439\n",
       "climdiv        439\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = stations_df[stations_df.station_uid.isin(observations_df['station_uid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_uid    27\n",
       "name           27\n",
       "county         27\n",
       "state          27\n",
       "latitude       27\n",
       "longitude      27\n",
       "climdiv        27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_uid      int64\n",
       "name            object\n",
       "county          object\n",
       "state           object\n",
       "latitude       float64\n",
       "longitude      float64\n",
       "climdiv         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create calculated fields for dashboard and ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns to observation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a freeze_day column based on min temperature\n",
    "observations_df['freeze_day'] = np.where(observations_df['mint'] <= 32, 1, 0)\n",
    "\n",
    "# Flag if it reaches above freezing on the day\n",
    "observations_df['above_freezing'] = np.where(observations_df['maxt'] > 32, 1, 0)\n",
    "\n",
    "# Create columns for the different date parts to make processing easier\n",
    "observations_df['obs_year'] = pd.to_datetime(observations_df['date']).dt.year\n",
    "observations_df['obs_month'] = pd.to_datetime(observations_df['date']).dt.month\n",
    "observations_df['obs_day'] = pd.to_datetime(observations_df['date']).dt.day\n",
    "observations_df['obs_dayofyear'] = pd.to_datetime(observations_df['date']).dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station/Year dataframe creation and calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to store yearly summary info by station ID\n",
    "years = pd.to_datetime(observations_df['date']).dt.year.unique()\n",
    "years_df = pd.DataFrame(years,columns=['obs_year'])\n",
    "station_yearly_metrics_df = pd.merge(stations_df['station_uid'], years_df, how='cross')\n",
    "station_yearly_metrics_df = station_yearly_metrics_df.set_index(['station_uid','obs_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last frost date of each station for each year of data\n",
    "last_freeze_df = observations_df.loc[(observations_df['freeze_day']==1)  & (observations_df['obs_dayofyear'] < 180),['station_uid','date','obs_year','obs_dayofyear'] ]. \\\n",
    "        groupby([\"station_uid\",\"obs_year\"])[['date','obs_dayofyear']].max().rename(columns={'date':'last_freeze_date','obs_dayofyear':'last_freeze_dayofyear'})\n",
    "\n",
    "# get the first freeze in the fall\n",
    "first_freeze_df = observations_df.loc[(observations_df['freeze_day']==1)  & (observations_df['obs_dayofyear'] >= 180),['station_uid','date','obs_year','obs_dayofyear'] ]. \\\n",
    "        groupby([\"station_uid\",\"obs_year\"])[['date','obs_dayofyear']].min().rename(columns={'date':'first_freeze_date','obs_dayofyear':'first_freeze_dayofyear'})\n",
    "\n",
    "# Determine if we have a complete set of observations for april to may for each station/year\n",
    "april_to_may_days_recorderd_df = pd.DataFrame(observations_df.loc[(observations_df['obs_month']>=4 )&(observations_df['obs_month'] <= 6),['station_uid','obs_year','mint']]\\\n",
    "        .groupby(['station_uid','obs_year'])['mint'].count()).rename(columns={'mint':'observations_recorded_april_to_may'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>coldest_day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_uid</th>\n",
       "      <th>obs_year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10392</th>\n",
       "      <th>2002</th>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>-14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      coldest_day\n",
       "station_uid obs_year             \n",
       "10392       2002             -5.0\n",
       "            2003            -16.0\n",
       "            2004            -11.0\n",
       "            2005            -16.0\n",
       "            2006            -14.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Get the coldest day of the year\n",
    "coldest_day_of_year = observations_df.groupby([\"station_uid\",\"obs_year\"])[['mint']].min().rename(columns={'mint':'coldest_day'})\n",
    "coldest_day_of_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_uid</th>\n",
       "      <th>obs_year</th>\n",
       "      <th>coldest_day</th>\n",
       "      <th>coldest_dayofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10392</td>\n",
       "      <td>2002</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10392</td>\n",
       "      <td>2003</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10392</td>\n",
       "      <td>2004</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10392</td>\n",
       "      <td>2005</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10392</td>\n",
       "      <td>2006</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_uid  obs_year  coldest_day  coldest_dayofyear\n",
       "0        10392      2002         -5.0                 18\n",
       "1        10392      2003        -16.0                 38\n",
       "2        10392      2004        -11.0                359\n",
       "3        10392      2005        -16.0                 14\n",
       "4        10392      2006        -14.0                 49"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coldest_day_df = pd.merge(coldest_day_of_year, observations_df, how='left', left_on=['station_uid', 'obs_year','coldest_day'], right_on = [\"station_uid\",\"obs_year\",\"mint\"])\n",
    "coldest_day_of_year_df = pd.DataFrame(coldest_day_df.groupby([\"station_uid\",\"obs_year\",'coldest_day'])['obs_dayofyear'].max())\n",
    "coldest_day_of_year_df.rename(columns={'obs_dayofyear':'coldest_dayofyear'},inplace=True)\n",
    "coldest_day_of_year_df = coldest_day_of_year_df.reset_index()\n",
    "coldest_day_of_year_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldest_day_of_year_df = coldest_day_of_year_df.set_index(keys=['station_uid','obs_year'])\n",
    "station_yearly_metrics_df = pd.merge(station_yearly_metrics_df, coldest_day_of_year_df, how='left', left_index=True, right_index =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_yearly_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = station_yearly_metrics_df.loc[(station_yearly_metrics_df['station_uid'] == 40882)]\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hottest day of the year, if there are multiple days with the temperature, use the latest one in the year (the one closest to the next last freeze date the next spring)\n",
    "hottest_day_of_year = observations_df.groupby([\"station_uid\",\"obs_year\"])[['maxt']].max().rename(columns={'maxt':'hottest_day'})\n",
    "hottest_day_df = pd.merge(hottest_day_of_year, observations_df, how='left', left_on=['station_uid', 'obs_year','hottest_day'], right_on = [\"station_uid\",\"obs_year\",\"maxt\"])\n",
    "hottest_day_of_year_df = pd.DataFrame(hottest_day_df.groupby([\"station_uid\",\"obs_year\",'hottest_day'])['obs_dayofyear'].max())\n",
    "hottest_day_of_year_df.rename(columns={'obs_dayofyear':'hottest_dayofyear'},inplace=True)\n",
    "hottest_day_of_year_df = hottest_day_of_year_df.reset_index()\n",
    "hottest_day_of_year_df = hottest_day_of_year_df.set_index(keys=['station_uid','obs_year'])\n",
    "station_yearly_metrics_df = pd.merge(station_yearly_metrics_df, hottest_day_of_year_df, how='left', left_index=True, right_index =True)\n",
    "#hottest_day_of_year_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_yearly_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the yearly data \n",
    "station_yearly_metrics_df = pd.merge(station_yearly_metrics_df, last_freeze_df, how='left', left_on=['station_uid','obs_year'], right_index=True) #,  left_on=['station_uid', 'year'], right_on = [\"station_uid\",\"obs_year\"])\n",
    "station_yearly_metrics_df = pd.merge(station_yearly_metrics_df, first_freeze_df, how='left', left_on=['station_uid','obs_year'], right_index=True) # ,  left_on=['station_uid', 'year'], right_on = [\"station_uid\",\"obs_year\"])\n",
    "station_yearly_metrics_df = pd.merge(station_yearly_metrics_df, april_to_may_days_recorderd_df, how='left', left_on=['station_uid','obs_year'], right_index=True) # ,  left_on=['station_uid', 'year'], right_on = [\"station_uid\",\"obs_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_yearly_metrics_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of calculations for the Stations dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the mean/average last freeze date for a station \n",
    "avg_last_freeze_df = pd.DataFrame(station_yearly_metrics_df.groupby(['station_uid'])['last_freeze_dayofyear'].mean().round(0)).rename(columns={'last_freeze_dayofyear':'avg_last_freeze_dayofyear'})\n",
    "\n",
    "# Convert the day of year to a string value for mm/dd\n",
    "avg_last_freeze_df[\"avg_last_freeze_mm_dd\"] = pd.to_datetime(avg_last_freeze_df[\"avg_last_freeze_dayofyear\"],format='%j').dt.strftime('%m/%d')\n",
    "\n",
    "# determine the mean, get the string value\n",
    "median_last_freeze_df = pd.DataFrame(station_yearly_metrics_df.groupby(['station_uid'])['last_freeze_dayofyear'].median().round(0)).rename(columns={'last_freeze_dayofyear':'median_last_freeze_dayofyear'})\n",
    "median_last_freeze_df[\"median_last_freeze_mm_dd\"] = pd.to_datetime(median_last_freeze_df[\"median_last_freeze_dayofyear\"],format='%j').dt.strftime('%m/%d')\n",
    "\n",
    "# Merge the values into a single table\n",
    "station_metrics_df = pd.merge(stations_df, avg_last_freeze_df, left_on=['station_uid'], right_on = ['station_uid'])\n",
    "station_metrics_df = pd.merge(station_metrics_df, median_last_freeze_df, left_on=['station_uid'], right_on = ['station_uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_yearly_metrics_df.reset_index(inplace=True)\n",
    "# station_yearly_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the station metrics and station/yearly DF to determine metrics for each station\n",
    "merged_station_and_yearly_df = pd.merge(station_yearly_metrics_df, station_metrics_df, how=\"left\", on=[\"station_uid\", \"station_uid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of how many years the station is in the dataset\n",
    "stations_years_count = merged_station_and_yearly_df.groupby(\"station_uid\").count()[\"obs_year\"]\n",
    "\n",
    "# Calculate the number of years where the last freeze was before or on the average date\n",
    "stations_count_at_or_before_avg_last_freeze = merged_station_and_yearly_df[(merged_station_and_yearly_df[\"last_freeze_dayofyear\"] <= merged_station_and_yearly_df['avg_last_freeze_dayofyear'])]\n",
    "stations_count_at_or_before_avg_last_freeze = stations_count_at_or_before_avg_last_freeze.groupby(\"station_uid\").count()[\"obs_year\"]\n",
    "\n",
    "# Calculate the number of years where the last freeze was after the average date\n",
    "stations_count_later_than_avg_last_freeze = merged_station_and_yearly_df[(merged_station_and_yearly_df[\"last_freeze_dayofyear\"] > merged_station_and_yearly_df['avg_last_freeze_dayofyear'])]\n",
    "stations_count_later_than_avg_last_freeze = stations_count_later_than_avg_last_freeze.groupby(\"station_uid\").count()[\"obs_year\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_calc_values_df = pd.DataFrame(\n",
    "          {\"years_included\": stations_years_count,\n",
    "          \"count_at_or_before_avg_last_freeze\": stations_count_at_or_before_avg_last_freeze, \n",
    "          \"count_later_than_avg_last_freeze\": stations_count_later_than_avg_last_freeze})\n",
    "# station_calc_values_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_metrics_full_df = pd.merge(station_metrics_df, station_calc_values_df, how=\"left\", on=[\"station_uid\", \"station_uid\"])\n",
    "# station_metrics_full_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of Observation Day Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_calcs_loop(date_range):\n",
    "    counter = 0\n",
    "    observation_calcs_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through all the observations and calculate the metrics based on the number of days previous passed in\n",
    "    for index, row in observations_df.iterrows():\n",
    "        # call function to get previous X days of data\n",
    "\n",
    "        df = pd.DataFrame(observations_df.loc[(observations_df.station_uid == row['station_uid']) & \\\n",
    "                            (observations_df.date < row['date']) & \\\n",
    "                            (observations_df.date >= row['date'] - timedelta(days=date_range))] \\\n",
    "                    .groupby('station_uid').agg(max_temp=('maxt', 'max'), \n",
    "                            min_temp=('mint', 'min'),\n",
    "                            avgt=('avgt','mean'),\n",
    "                            precip=('pcpn','sum'),\n",
    "                            count=('date','count')) \\\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "        df['source_index'] = index\n",
    "\n",
    "        observation_calcs_df = pd.concat([observation_calcs_df,df], ignore_index=True)\n",
    "        \n",
    "    # Rename the columns for the range\n",
    "    column_array = [f'maxt_{date_range}_day', f'mint_{date_range}_day', f'avgt_{date_range}_day', f'precip_{date_range}_day',f'obs_count_{date_range}_day','source_index']\n",
    "    observation_calcs_df.columns = column_array\n",
    "\n",
    "    observation_calcs_df = observation_calcs_df.set_index(\"source_index\")\n",
    "    return observation_calcs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function for 7 days and merge the new columns into the table\n",
    "observations_calcs_7_day = observation_calcs_loop(7)\n",
    "observations_new_df = pd.merge(observations_df, observations_calcs_7_day, how=\"left\", left_index=True, right_index=True)\n",
    "# observations_new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function for 30 days and merge the new columns into the table\n",
    "observations_calcs_30_day = observation_calcs_loop(30)\n",
    "observations_new_df = pd.merge(observations_new_df, observations_calcs_30_day, how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Output file (CSV\n",
    "output_station_file = \"../Resources/station_data.csv\"\n",
    "output_station_year_file = \"../Resources/station_year_data.csv\"\n",
    "output_observation_file = \"../Resources/observation_data.csv\"\n",
    "\n",
    "station_metrics_full_df.to_csv(output_station_file, index=False)\n",
    "station_yearly_metrics_df.to_csv(output_station_year_file, index=False)\n",
    "observations_new_df.to_csv(output_observation_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL movie_data DB\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/last_freeze_analysis\"\n",
    "\n",
    "# Create the database engine with the following line \n",
    "engine = create_engine(db_string)\n",
    "\n",
    "# These two drop statements are to cleanup an old set of tablenames\n",
    "with engine.connect() as con:\n",
    "    con.execute(\"DROP TABLE IF EXISTS stations;\")\n",
    "with engine.connect() as con:\n",
    "    con.execute(\"DROP TABLE IF EXISTS observations;\")\n",
    "\n",
    "# Drop the view since we'll recreate it later\n",
    "with engine.connect() as con:\n",
    "    con.execute(\"DROP VIEW IF EXISTS v_days_until_freeze_calcs_0_to_180_days;\")\n",
    "\n",
    "# RECREATE THE TABLES WITH DATA.  CREATE IN THIS ORDER TO DEAL WITH FK's\n",
    "# Save the observations DataFrame to a SQL table \"observations\"- Replace the table if it already exists\n",
    "observations_new_df.to_sql(name='observation', con=engine, if_exists='replace', index=False)   \n",
    "\n",
    "# Save the stations DataFrame to a SQL table \"stations\"- Replace the table if it already exists\n",
    "station_yearly_metrics_df.to_sql(name='station_yearly', con=engine, if_exists='replace', index=False)   \n",
    "\n",
    "# Save the stations DataFrame to a SQL table \"stations\"- Replace the table if it already exists\n",
    "station_metrics_full_df.to_sql(name='station', con=engine, if_exists='replace', index=False)   \n",
    "\n",
    "\n",
    "\n",
    "# Add primary keys\n",
    "with engine.connect() as con:\n",
    "    con.execute(\"ALTER TABLE station ADD PRIMARY KEY (station_uid);\")\n",
    "\n",
    "with engine.connect() as con:\n",
    "    con.execute(\"ALTER TABLE station_yearly ADD PRIMARY KEY (station_uid,obs_year);\")\n",
    "\n",
    "with engine.connect() as con:\n",
    "    con.execute(\"ALTER TABLE observation ADD PRIMARY KEY (station_uid,date);\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view for additional calculations\n",
    "with engine.connect() as con:\n",
    "    con.execute(\"CREATE OR REPLACE VIEW v_days_until_freeze_calcs_0_to_180_days \\\n",
    "                AS SELECT o.station_uid, o.date, s.avg_last_freeze_dayofyear - o.obs_dayofyear AS days_until_avg_freeze_dayofyear, \\\n",
    "                sy.last_freeze_dayofyear - o.obs_dayofyear AS days_until_current_year_last_freeze_dayofyear \\\n",
    "                FROM observation o \\\n",
    "                JOIN station_yearly sy ON sy.station_uid = o.station_uid AND sy.obs_year = o.obs_year \\\n",
    "                JOIN station s ON sy.station_uid = s.station_uid \\\n",
    "                WHERE o.obs_dayofyear >= 0 AND o.obs_dayofyear < 180;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Add foreign keys\n",
    "#### not adding an FK to this reporting table since it's a reporting data, not source data\n",
    "# with engine.connect() as con:\n",
    "#     con.execute(\"ALTER TABLE station_yearly ADD CONSTRAINT stn_yrly_station_uid_fk FOREIGN KEY (station_uid) REFERENCES station (station_uid);\")\n",
    "\n",
    "with engine.connect() as con:\n",
    "    con.execute(\"ALTER TABLE observation ADD CONSTRAINT obs_station_uid_fk FOREIGN KEY (station_uid) REFERENCES station (station_uid);\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of script\n"
     ]
    }
   ],
   "source": [
    "print('end of script')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79cce83be72e55c1187b298660b26fa831a7f54746f9fd5c4205698cac5dca1a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
